问题背景：
问题优点：无接触，难以隐藏，相比于虹膜和指纹，步态无需配合，无控制场合  当下的街头，车站，机场，商场以及办公室等等地方都有布局大量的摄像头24小时不间断的监控着经过的人群，有摄像头的地方就有步态识别

问题难点：
两个部分：
主体相关因素：速度，衣着，背包种类和外形，姿态，自遮挡（极度影响识别率）
设备相关因素：摄像质量，帧速，分辨率
环境相关因素：照明条件，拍摄视角---视角影响最大。
如果一种算法忽视视角可能引发的问题，其效果可能随着视角的改变产生巨大的下降。例如：当不考虑视角不同可能产生的问题之时，对法国国家信息与自动化研究所（INRIA）的IXMAS数据集[3’]识别的识别率会从80%下降到低于20%[4’]。原因在于：当视角差达到一定程度之时，主体的外形会被彻底的改变，导致类内变化大于类间变化。
历史上对交叉视角问题的处理方法：
方法一：重建人体3D模型
重建人体的3D模型之后通过映射可以生成任意想要的2D模型[7’][8’]，这种方法可以获得相当客观的效果，但通常需要多个同步协作且完全受控的摄像机一同拍摄，在现实生活当中很多时候只有2D摄像机可以用，所以这种方法很难普及和应用于社会当中。
方法二：使用手工的视角不变量特征[9’][10’]
这种方法在特定场合之下的效果不错，但是很难泛化到一般性场合。
方法三：使用模型或机器学习方法进行视角映射[11’][12’]
这种方法将一个视角下的步态特征标准化到另一个信息量更全面的视角当中，通常是侧视角。所以将两个视频当中的两组步态视频提取出，标准化到侧视角之后比对相似性即可。经验结果表明：这类方法在视角差较小的时候可以取得不错的效果（通常是不大于18°），然而当视角差大于36°之时，这类方法的效果会下降很多。	


视角问题：Kale et al 使用视角映射模型从任意视角侧视角，然而两个视角之间的联系很难用类似于PPM这样的简单线性模型来模拟。[10]
比较流行的转换模型： VTM（视角转换模型） Makihara等人提出了基于SVD的FD-VTM应用于时间领域[15]。Kusakunniran等人[33’]使用简化版的SVD来避免VTM会出现的空间复杂度过大和过拟合问题。RSVD-VTM（）应用于空间领域[11].分支三号：基于鲁棒性主成分分析（RPCA），Zheng等人提出了鲁棒性VTM用来提取视角不变特征[23]。相比于将步态特征投影到一个公共空间当中，Bashir等人[36’]使用标准相关性分析(Canonical Correlation Analysis, CCA)将每一对步态特征投影到两个具有最大关联的子空间当中。
对于大多数视角转换模型，单一视角转换模型只能把某一特定视角转换为另一特定视角，[2,3,11,12,15,23],因此严重依赖于视角评估的准确性，如果想把步态图像从任意角度转换为特定角度，需要很多很多的模型。一些其他学者研究了只用一个模型来实现识别功能，Hu等人[27’]提出一个利用统一线性映射，名为视角不变性差异映射（View-invariant Discriminative Projection, ViDP）的方法，利用线性转换实现视角不变特征的提取。Wu等人训练深度卷积神经网络处理任意视角并取得了很好的研究效果。
衣着问题：尤其可以遮住腿的外套。相比于视角不变量，以往尝试衣着不变量的的方法很少。在[7]中，通过将人体分成8个服从判别分析的部分来实现。在[5]中，Guan等人提出随机子空间方法，通过组合多个归纳偏置用于分类。

最近的工作：
在最近的文献当中，步态识别方法可以大致分成两类：基于模型的方法和基于外形的方法。前者将人体的内在结构模型化，而后者从视频当中直接提取步态特征，不会考虑内在的结构。考量到在很多场合之中精确的恢复人体结构并不是一件容易的事情，本文使用的是基于外形的方法。
基于外形的方法有一个基本的流程结构：首先从一个视频当中提取出所有包含主体镜头的帧，通过图像尺寸调节和裁剪之后成为一组组图像集，将图像集里面所有的图像在空间维度排列起来，随后在时间维度计算其平均图产生目标的步态能量图。最后比较不同步态能量图（GEI）中的信息，例如计算欧几里得距离（Euclidean Distance），得到最终的结果。
在步态识别领域中有很多可以代替步态能量图的选择，例如长期步态图（Chrono-gait Image, CGI）[31’]以及步态流图（Gait Flow Image）[32’]。但是最近一项由Iwama等人的研究表明，尽管步态能量图非常朴素，但他们在4007个数据之上进行的实验结果显示步态能量图最为稳定且效果最好。
如前面提到，交叉视角问题的解决方案有三种。第一种方法是重建人体的3D结构，第二种方法是基于手工的视角不变特征来解决，本文所使用的方法属于第三种方法，也就是通过模型或机器学习的方法进行视角映射。通过学习好的映射模块，处于不同视角之下的步态特征可以被映射到特定的公共子空间当中做更准确的比对。相比于前两种交叉视角问题的解决方案，第三种方法可以应用于无需主体配合的场合，并且可以直接应用于和侧视角明显不同的角度，例如正视角(0°)和反视角(180°)等。

问题展望：随着识别率的提高可以很好的应用于犯罪预防，司法鉴定
技术特点：双判别器（真假判别，身份判别）
技术难点：生成不变量步态图的过程中如何保留有用的身份信息
技术优点：视角和其他不确定因素在生成步态图之前不需要提前准备
实现目标：生成90°侧视角，伴随通常穿着没有背包
技术：
GAN:
博弈论中的双人零和。Goodfellow等人在2014年搞得[4]。生成器捕获训练数据的分布，用一个随机向量z产生图片。判别器从训练集中的真实数据和生成器中生成的虚假数据中随机选定一个作为输入，并且评估输入是真的还是生成的。两个模型彼此互相优化的过程是一个最小最大双玩家博弈问题。
由于GAN很难训练，生成器经常产生没有意义的输出。历史上研究人员提出过很多方法来进行约束和优化。Mirza等人将一个条件参数y作为新增的输入层输入到判别器和生成器中用来增加对模型的限制[16]。Radford等人提出了拥有一系列策略的深度卷积对抗式生成网络(DCGAN,Deep Convolution Generative Adversarial Networks)来稳定训练过程，包括反卷积(Deconvolution)，又名小数步长卷积（Fractional Strided Convolution）以及标准化处理过程Batch Normalization[17]。最近Yoo等人提出了一个基于DCGAN的变种模型，其生成器是以图像为条件的，器判别器包含了一个名为领域判别器(Domain Discriminator)的重要组成部分，该判别器确保被生成的图片和输入是相关的。该模型名为像素级领域转换生成式对抗网络（PixelDTGAN,Pixel-Level Domain Transfer Generative Adversarial Network）。和以往的GAN相关工作相比，不只是可以生成看起来真实的图片，生成器生成的图片可以帮助提升判别器性能。
GEI:	
步态能量图（GEI）是一个流行的步态特征，它是由步态序列中一个步态周期内的图像平均化得到的。对噪声鲁棒性很强并且便于计算。在步态能量图中，每一个像素块颜色的深浅，或者说像素值的高低代表了一个步态周期之中此处被人体占据的时间比率。在提出的方法之中，步态能量图是输入和目标。
   
PixelDTGAN：                       
在常规GAN中，生成器的最终目标是将一个小维度空间向量z映射到像素级图像空间。在PixelDTGAN中，生成模型的输入不再是一个向量z而是一个图像。此时的GAN实现了输入图像和目标图像之间像素级领域转换。该神经网络模拟了由眼睛看到的现实景象转化为大脑中想象的心理图像的过程。作者在该模型中定义了两个领域，源领域和目的领域，并且通过语义学内涵将两个领域串联起来。举例说明：假设源领域是一个穿着衣服的人物图像，那么目的领域将是这个人物所穿着的衣服的图像。所生成的图像不仅仅应当看起来是真实的并且保留着语义学内涵。在整个模型中，真假判别器用于确保生成的图像是真实的，领域判别器用于确保生成的图像包涵了语义学信息。
剩下的第三部分是一个由一对用于嵌入语义学内涵的编码器和产生目标图像的解码器组成的像素级转换器组成，两部分都是基于卷积神经网络。在转换器的顶端需要一些类似于损失函数的用来限制输出的策略来控制产生的图像，因此Yoo等人在转换器顶端连接了一个上文提到的领域转换器。
领域判别器接收一对源图像和目标图像作为输入，并可以产生一个标定概率来表明一对输入图像是否具有联系。领域转换器的损失函数被定义为：					         	 
是源图像，是真实的目标图像，是由转换器生成的图像而是不相关的目标图像。
另一个判别器——真假判别器类似于传统生成式对抗网络中的判别器，由一对真假标签所监督，目的是为了整个系统可以产生真实的图像。类似于领域判别器，真假判别器也会产生一个标定概率来表明转换器生成的图像是否是真实的，领域判别器的损失函数以二进制交叉熵的形式定义：

包含了真实的图像集，包含了由生成器生成的虚假图像集。
所有的标签都送到两个判别器当中，而后判别器用于监督转换器产生看起来真实并且包含着语义学内涵的图像。
GaitGAN：
受PixelDTGAN的启发，在此提出名为GaitGAN的神经网络。该神经网络将任意角度下拍摄的伴随任意装着和背包条件的目标步态数据转换为处于90°侧视角下穿着通常服装且去除背包条件的目标步态数据，除此之外也保留着目标的身份信息。
在真假判别器的监督之下，转换器可以生成看起来真实的图像，但是图像主体是否是同一个人并不能保证。因此，类似于PixelDTGAN中的领域判别器，在此提出身份判别器，将一张源图像和一张目标图像输入其中，产生一个标定概率来表明源图像和目标图像是否是同一个人。如果两张输入的图像来自于同一主体，那么判别器会输出1。如果两张输入的图像来自于不同的主体，判别器会返回0。除此之外，如果一张图像是源图像，另一张图像是由转换器生成的目标图像，身份判别器依旧会返回0。
OU-ISIR[28’]：
目前最大的可用数据库，由大阪大学科学和工业研究院构建。包含4007个主体，年龄跨度从1岁到94岁，男女比例接近1:1。

USF[29’]：
由南佛罗里达州大学提出其内的步态序列在真实的室外环境拍摄而成。

CASIA-B数据集：
CASIA-B数据集是由中国国家科学院自动化所于2005年1月创作的，是当今世界上较大的步态数据集之一。整个数据集包含124个主体，其中31个为女性，93个为男性。每个主体有10个拍摄序列，包括6个常规步行序列，2个背包步行序列以及2个穿着外套的步行序列，每一个拍摄序列由11个角度构成，每两个角度之间有18°的角度差。

实验设计：
一共有124组主体序列，将所有主体一分为二。前62组主体被设置为训练集，后62组主体被设置为测试集。在测试集中，每个主体的后四个常规步行序列被放入标准集中，前两个常规序列以及剩下的四个序列被放入探针集当中。
训练集	No.001~062  nm01~nm06, bg01~bg02, cl01~cl02
标准集	No.063~124  nm05~nm06

探针集	Normal Walking	No.063~124  nm01~nm02
	Walking With Bag	No.063~124  bg01~bg02
	Walking With Coat	No.063~124  cl01~cl02   

此处说明整个模型三部分的结构，首先是最底层的转换器。转换器可以被分为两个部分，分别是编码器和解码器。编码器由4个卷积层构成，将源图像抽象到另一个完全捕获了目标步态特征的空间。而后产生的特征向量z进入解码器中，经过4个解码之后会被转换为一个相关的目标。每个解码层都是一个小数步长卷积层，也就是卷积在相反的方向进行。剩下的两部分分别是身份判别器和真假判别器，其内部结构与编码器的四个卷积层是相似的，而经过这四个卷积层之后，两个判别器连接到第五层网络产生一个二进制数值。
卷积层号	卷积核数量	卷积核尺寸	步长	Batch Normalization	激活函数
1	96	4*4*{1,1,2}	2	NO	L-ReLU
2	192	4*4*96	2	YES	L-ReLU
3	384	4*4*192	2	YES	L-ReLU
4	768	4*4*384	2	YES	L-ReLU
编码器的详细结构：
解码器的详细结构：
反卷积层号	卷积核数量	卷积核尺寸	步长	Batch Normalization	激活函数
1	768	4*4*384	0.5	YES	L-ReLU
2	384	4*4*192	0.5	YES	L-ReLU
3	192	4*4*96	0.5	YES	L-ReLU
4	96	4*4*1	0.5	NO	Tanh

可以看到随着训练轮数的增加，无论是通常情况下的步行或是穿着外套，背着背包的情况识别率都在不断变化。纵观三条识别率的折线图，三种情况下的识别率大概在450轮训练次数之时达到最高。所以神经网络的训练轮数最终被定为450次。








	探针集（normal walking   nm01 & nm02）
	0	18	36	54	72	90	108	126	144	162	180



标
准
集

	0°	100.0  	79.03 	45.97	37.23 	28.23 	23.27 	26.61 	27.33 	31.45 	54.84 	72.58
	18°	78.23  	99.19 	91.94 	70.14	46.77 	36.93 	37.90 	42.17 	43.55 	65.32 	58.06
	36°	56.45	88.71	97.58	91.27	75.00	53.77	59.68	71.82	70.16	60.48	35.48
	54°	33.87 	53.23 	85.48 	96.51 	87.10 	76.03 	75.00 	73.18 	63.71 	37.10 	22.58
	72°	27.42 	41.13 	69.35 	86.15 	100.0 	95.32 	89.52 	75.10 	62.10 	37.10 	17.74
	90°	22.58 	37.10 	54.84 	77.41 	98.39 	98.44	96.77 	74.22 	57.26 	35.48 	21.77
	108°	20.16   	32.26 	58.06	71.44	90.32	93.31 	97.58 	93.22 	74.19 	38.71 	22.58
	126°	29.84 	37.90 	66.94 	73.31 	81.45 	77.81 	91.13 	99.49 	97.58 	59.68 	37.10
	144°	28.23    	45.97 	60.48 	63.27	61.29 	60.18 	75.00	96.21 	99.19	79.84 	45.97
	162°	48.39    	63.71 	60.48	51.40 	41.13	35.15 	45.16 	67.38 	83.06	99.19 	71.77
	180°	73.39   	56.45 	36.29	25.81	21.77 	19.35 	20.16 	31.45 	44.35 	72.58 	100.0

	探针集（walking with bag   bg01 & bg02）
	0°	18°	36°	54°	72°	90°	108°	126°	144°	162°	180°



标
准
集	0°	79.03      	45.97 	33.06	14.52	16.13 	14.52	11.29  	15.32  	22.58  	33.87  	41.13
	18°	54.84 	76.61  	58.87  	31.45 	26.61  	16.13  	24.19  	29.84  	32.26  	41.94  	32.26  
	36°	36.29 	58.87 	75.81 	53.23 	44.35 	30.65 	34.68 	46.77 	42.74 	34.68 	20.16
	54°	25.00  	45.16  	66.13  	68.55  	57.26 	42.74  	41.13  	45.97  	40.32  	20.16 	13.71
	72°	20.16    	24.19	38.71  	41.94 	65.32	56.45  	57.26  	51.61 	39.52 	16.94  	8.87 
	90°	15.32 	27.42  	37.90  	38.71  	62.10  	64.52  	62.10 	61.29  	38.71 	20.97  	12.10 
	108°	16.13  	25.00 	41.13	42.74  	58.87 	58.06  	69.35  	70.16  	53.23  	24.19  	11.29   
	126°	19.35  	29.84  	41.94  	45.16 	46.77  	52.42  	58.06  	73.39  	66.13 	41.13 	22.58
	144°	26.61  	32.26  	48.39 	37.90 	37.10  	36.29  	38.71 	67.74 	73.39 	50.00  	32.26   
	162°	29.03     	34.68  	36.29 	25.00 	19.35 	16.13  	20.16 	37.90  	51.61 	76.61 	41.94
	180°	42.74   	28.23  	24.19 	12.90 	11.29  	11.29  	14.52 	21.77  	30.65  	49.19 	77.42  

	探针集（walking with coat   cl01 & cl02）
	0°	18°	36°	54°	72°	90°	108°	126°	144°	162°	180°



标
准
集	0°	25.81   	16.13  	15.32	12.10 	6.45 	6.45 	9.68  	7.26   	12.10   	11.29   	15.32
	18°	17.74 	37.90   	34.68   	20.97 	13.71   	8.87   	12.10   	19.35  	16.94   	24.19   	19.35  
	36°	13.71  	24.19 	45.16  	43.55 	30.65 	19.35  	16.94  	22.58  	28.23  	20.16  	10.48 
	54°	2.42   	19.35  	37.10   	55.65   	39.52  	22.58  	29.03   	29.84   	29.84 	16.94 	8.06
	72°	4.84      	12.10 	29.03   	40.32  	43.55	34.68   	32.26  	28.23 	33.87  	12.90   	8.06  
	90°	4.03   	10.48   	22.58   	31.45   	50.00  	48.39  	43.55  	36.29  	31.45 	13.71   	8.06
	108°	4.03  	12.90  	27.42 	27.42   	38.71  	44.35   	47.58   	38.71  	32.26  	15.32   	4.84 
	126°	10.48 	10.48   	23.39   	27.42  	26.61   	25.81   	37.10   	45.97 	41.13 	15.32  	10.48  
	144°	8.87 	13.71   	26.61  	22.58  	18.55   	19.35   	21.77  	35.48  	43.55  	20.97  	12.90   
	162°	14.52     	18.55   	20.97  	17.74  	12.10  	12.10   	17.74  	21.77   	37.10 	35.48  	21.77 
	180°	17.74   	13.71   	11.29  	6.45  	10.48   	5.65  	6.45 	5.65  	14.52   	29.03  	27.42    

三个表格是在训练过神经网络之后经过测试后返回的数据汇总。作为对照的标准集为CASIA-B数据集中63~124号主体的nm03~nm06中的步态图像。自上而下的三个表格的测试集内容分别为63~124号主体的nm01~nm02中的步态图像，63~124号主体的bg01~bg02中的步态图像以及63~124号主体的cl01~cl02中的步态图像。因每一组图像拍摄的角度从0到180度一共有11组图像，所以一共生成了11个步态能量图，最终的表格数据项共有121项。
与其他方法的比较：
为了将目前提出的方法和历史上已经提出来的各种方法的识别率进行横向对比分析，在相关文献中寻找到了以下方法的识别率数据：FD-VTM[15],RSVD-VTM[11],RPCA-VTM[23],GP+CCA[1],R-VTM[12],SPAE[21]以及C3A[19]。探针集的角度被选定为54，90以及126度。以上提到的7种方法和此处提到的方法一共8种方法的识别率横向比较图如下所示：



从图表上可以看出：上述方法在探针集和标准集之间的视角差不是很小的时候（大于36°之时）通常都会领先于绝大多数其他步态识别方法的识别率，这表明上述方法面对不是很小的角度差之时有着相当的优越性。当视角差局限在左右偏置36°之内的时候，上述方法的识别率在多余半数的情况下处于所有方法的前列。因此，纵观目前比较常见的步态识别方法，上述提到的方法在各种情况之下总的识别率领先于绝大多数其他方法。

	探针集角度
	54°	90°	126°	平均值
C3A[19]	56.6%	54.7%	58.4%	56.6%
SPAE[21]	63.3%	62.1%	66.3%	63.9%
CNN[18]	77.8%	64.9%	76.1%	72.9%
Proposed 	64.7%	57.1%	65.2%	62.3%
说明：该表格为当探针集处于54°，90°及126°之时四种方法的平均识别率。对应的标准集角度为除探针集所在角度的对应角度之外的其他十种角度。自上而下的四种方法分别是：C3A,SPAE,CNN以及本文提到的方法。最后一列的平均值为四种方法在三种角度之下识别率的平均值。

左侧的图表展示了C3A[19],SPAE[21],CNN[18]以及本文提到的方法的识别率平均值。从数据上可以看到同时面对任意角度，背包和着装的条件之下，本文提到的仅仅使用了一个生成模型的识别方法仍然取得了不错的识别率。实验表明生成式对抗网络在步态识别研究中有不错的应用前景。
未来可以提升的空间：
无配合步态识别数据库的建立：
本文提到的方法以及目前绝大多数的步态识别方法所用于测试的数据集中的主体都有一定的配合，现在将目光转向没有任何配合的自然环境当中：摄像机所拍摄到的主体可能会摔倒，也可能会转头，这样所获得到的步态序列是不连续的。有些场合下同一时刻可能会拍到多个人物在图像范围内移动，甚至可能会拍摄到无关物体在移动，诸如此类的情况会在很大程度之上增大提取人物主体的难度，更会增大识别算法的识别难度，降低识别率。很多情况之下摄像机处在被拍摄对象的上方，因此更多具有可能性的视角应当被考虑到其中。如果我们想要将步态识别技术应用到社会之中，这些问题都是一定需要被解决的。然而在步态识别领域当中，几乎没有在上述方面做过研究的科学家，也没有相关的数据集可以用来研究。	不过在相关的文献中我们可以看到，有一些数据库被设计的初衷是应用于多摄像机人物跟踪或是人体重新识别等方面的研究，这些数据库在一定程度之上可以应用到步态识别当中。例如Bialkowski等人[53’]创造了一个多摄像机摄像网络的数据集，然而相比于CASIA-B中13640个数据，该数据集中只有150个序列。由于该数据集过小，在其之上训练的神经网络会由于严重的过拟合而不具有良好的泛化能力。自然而然的，截止到目前没有在步态识别的相关文献当中没有出现过在该数据集之上的步态识别结果，毕竟这本身就不是一个为了进行步态识别而设计的数据库。也许在未来会有相关的研究工作可以帮助我们实现步态识别实际应用的目标。
预处理过程优化：
有很多文献中已有的方法可以被用来提升预处理过程的速度和效果。例如行人检测方法(Pedestrian Detection Method, PDM)[54’]可以将人物从复杂的背景当中定位出来，姿势评估方法(Pose Estimation Method, PEM)[57’]可以提供辅助信息，精炼人物轮廓。如果没有这些方法，想要处理上述提到的无配合情况之下的步态数据库会非常棘手。但在本文当中，预处理过程并不是我们所关心的要点所在，因此这些一同被归为未来的工作。需要特别说明的是由于需要更为精准的定位，目前的行人检测方法需要更为复杂的调整才能应用到步态识别任务当中。
参考文献：
[1]K. Bashir, T. Xiang, and S. Gong. Cross-view gait recognition using correlation strength. In BMVC, 2010.
[2] X. Ben, W. Meng, R. Yan, and K. Wang. An improved biometrics technique based on metric learning approach. Neurocomputing, 97:44 – 51, 2012.
[3] X. Ben, P. Zhang, W. Meng, R. Yan, M. Yang, W. Liu, and H. Zhang. On the distance metric learning between crossdomain gaits. Neurocomputing, 208:153 – 164, 2016. SI: BridgingSemantic.
[4] I. J. Goodfellow, J. Pougetabadie, M. Mirza, B. Xu, D. Wardefarley, S. Ozair, A. Courville, Y. Bengio,
Z. Ghahramani, and M. Welling. Generative adversarial nets. Advances in Neural Information Processing Systems, 3:2672–2680, 2014.
[5] Y. Guan, C. T. Li, and Y. Hu. Robust clothing-invariant gait recognition. In 2012 Eighth International Conference on Intelligent Information Hiding and Multimedia Signal Processing, pages 321–324, July 2012.
[7] M. A. Hossain, Y. Makihara, J. Wang, and Y. Yagi. Clothinginvariant gait identification using part-based clothing categorization and adaptive weight control. Pattern Recognition,
43(6):2281 – 2291, 2010.
[10]A. Kale, A. K. R. Chowdhury, and R. Chellappa. Towards a view invariant gait recognition algorithm. In IEEE Conference on Advanced Video and Signal Based Surveillance, pages 143–150, July 2003.
[11] W. Kusakunniran, Q. Wu, H. Li, and J. Zhang. Multiple views gait recognition using view transformation model based on optimized gait energy image. In ICCV Workshops, pages 1058–1064, 2009.
[12] W. Kusakunniran, Q. Wu, J. Zhang, and H. Li. Gait recognition under various viewing angles based on correlated motion regression. IEEE TCSVT, 22(6):966–980, 2012.
[15]Y. Makihara, R. Sagawa, Y. Mukaigawa, T. Echigo, and Y. Yagi. Gait recognition using a view transformation model in the frequency domain. In ECCV, pages 151–163, 2006.
[16] M. Mirza and S. Osindero. Conditional generative adversarial nets. Computer Science, pages 2672–2680, 2014.
[17] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. Computer Science, 2015.
[18]Z. Wu, Y. Huang, L. Wang, X. Wang, and T. Tan. A comprehensive study on cross-view gait based human identification with deep cnns. IEEE TPAMI, 39(2):209–226, Feb 2017.
[19] X. Xing, K. Wang, T. Yan, and Z. Lv. Complete canonical correlation analysis with application to multi-view gait recognition. Pattern Recognition, 50:107–117, 2016.

[21] S. Yu, H. Chen, Q. Wang, L. Shen, and Y. Huang. Invariant feature extraction for gait recognition using only one uniform model. Neurocomputing, 239:81–93, 2017.
[23]S. Zheng, J. Zhang, K. Huang, R. He, and T. Tan. Robust view transformation model for gait recognition. In ICIP, pages 2073–2076, 2011.
